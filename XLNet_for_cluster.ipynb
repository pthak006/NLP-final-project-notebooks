{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ribfO5o-jZ5t",
        "outputId": "dc0be5a1-eab0-4a0a-cd09-7d2a6074e4b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgHfVuiQjO71"
      },
      "outputs": [],
      "source": [
        "# Load necessary libraries\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "Nqg3KX0xqDIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import urllib.request\n",
        "import tarfile\n",
        "\n",
        "# Download IMDB dataset\n",
        "url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "urllib.request.urlretrieve(url, \"aclImdb_v1.tar.gz\")\n",
        "\n",
        "# Extract IMDB dataset\n",
        "with tarfile.open(\"aclImdb_v1.tar.gz\", \"r:gz\") as tar:\n",
        "    tar.extractall()\n",
        "\n",
        "# Load IMDB dataset\n",
        "imdb_df = pd.DataFrame(columns=['review', 'sentiment'])\n",
        "for dataset in ['train', 'test']:\n",
        "    for sentiment in ['pos', 'neg']:\n",
        "        path = f'aclImdb/{dataset}/{sentiment}'\n",
        "        for filename in os.listdir(path):\n",
        "            with open(f'{path}/{filename}', 'r') as file:\n",
        "                review = file.read()\n",
        "            sentiment_value = 1 if sentiment == 'pos' else 0\n",
        "            imdb_df = imdb_df.append({'review': review, 'sentiment': sentiment_value}, ignore_index=True)\n"
      ],
      "metadata": {
        "id": "BnxHHWrGjUWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of instances\n",
        "num_instances = len(imdb_df)\n",
        "\n",
        "# Count the number of positive and negative instances\n",
        "num_positive = imdb_df['sentiment'].value_counts()[1]\n",
        "num_negative = imdb_df['sentiment'].value_counts()[0]\n",
        "\n",
        "# Print the results\n",
        "print(f'Total number of instances: {num_instances}')\n",
        "print(f'Number of positive instances: {num_positive}')\n",
        "print(f'Number of negative instances: {num_negative}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr78iN0tjixj",
        "outputId": "07c7a445-6ceb-400a-bc40-9ab2eff9ca1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of instances: 50000\n",
            "Number of positive instances: 25000\n",
            "Number of negative instances: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert sentiment values to numeric\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "imdb_df['sentiment'] = pd.to_numeric(imdb_df['sentiment'], errors='coerce')\n",
        "imdb_df = imdb_df.dropna()\n",
        "\n",
        "# Preprocess dataset\n",
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for review in imdb_df['review']:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        review,                     \n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 256,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(imdb_df['sentiment'].values, dtype=torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITf_7ByuLmZy",
        "outputId": "fb962a75-1bb5-4ad1-a337-15c550eb89d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training and validation sets\n",
        "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=42, test_size=0.2)\n",
        "train_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                                 random_state=42, test_size=0.2)"
      ],
      "metadata": {
        "id": "ppIgFxO_CFDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "epochs = 3"
      ],
      "metadata": {
        "id": "i7FIapr3QiNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define XLNet model for sequence classification\n",
        "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxc4PZOdQpwv",
        "outputId": "dffef4c2-0012-47f3-ae74-9323d759abc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device to run the model on\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_BpSsTPUq0L",
        "outputId": "c24579ca-b388-4cda-85ae-c33175879ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLNetForSequenceClassification(\n",
              "  (transformer): XLNetModel(\n",
              "    (word_embedding): Embedding(32000, 768)\n",
              "    (layer): ModuleList(\n",
              "      (0): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (6): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (7): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (8): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (9): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (10): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (11): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (sequence_summary): SequenceSummary(\n",
              "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "    (first_dropout): Identity()\n",
              "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
      ],
      "metadata": {
        "id": "UQinrMxCV7Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DataLoader for training data\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Create the DataLoader for validation data\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "7jENNbm1WDFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training loop\n",
        "for epoch in range(epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Track the training loss and accuracy\n",
        "    total_train_loss = 0\n",
        "    total_train_accuracy = 0\n",
        "\n",
        "    # Iterate over the training data\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Clear the gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Move the batch to the device\n",
        "        batch_inputs = tuple(t.to(device) for t in batch)\n",
        "        inputs = {'input_ids': batch_inputs[0],\n",
        "                  'attention_mask': batch_inputs[1],\n",
        "                  'labels': batch_inputs[2]}\n",
        "\n",
        "        # Perform the forward pass\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Perform the backward pass and update the parameters\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track the training loss and accuracy\n",
        "        total_train_loss += loss.item()\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "        accuracy = (preds == inputs['labels']).float().mean()\n",
        "        total_train_accuracy += accuracy.item()\n",
        "\n",
        "        # Print training progress\n",
        "        if step % 50 == 0:\n",
        "            print(f'Epoch: {epoch + 1}, Batch: {step}, Training Loss: {total_train_loss / (step + 1)}, Training Accuracy: {total_train_accuracy / (step + 1)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmdj0RrsWKe2",
        "outputId": "d92f839d-c52f-4573-e7ce-23f4270d1fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Batch: 0, Training Loss: 0.6707586050033569, Training Accuracy: 0.5625\n",
            "Epoch: 1, Batch: 50, Training Loss: 0.46363530687841714, Training Accuracy: 0.7843137254901961\n",
            "Epoch: 1, Batch: 100, Training Loss: 0.38383136689662933, Training Accuracy: 0.8316831683168316\n",
            "Epoch: 1, Batch: 150, Training Loss: 0.3526092063980979, Training Accuracy: 0.8526490066225165\n",
            "Epoch: 1, Batch: 200, Training Loss: 0.32465390178064507, Training Accuracy: 0.8650497512437811\n",
            "Epoch: 1, Batch: 250, Training Loss: 0.304850274058749, Training Accuracy: 0.8742529880478087\n",
            "Epoch: 1, Batch: 300, Training Loss: 0.2930980690776609, Training Accuracy: 0.8799833887043189\n",
            "Epoch: 1, Batch: 350, Training Loss: 0.2789401008543211, Training Accuracy: 0.8856837606837606\n",
            "Epoch: 1, Batch: 400, Training Loss: 0.27349867563499625, Training Accuracy: 0.8877805486284289\n",
            "Epoch: 1, Batch: 450, Training Loss: 0.2654901701180541, Training Accuracy: 0.8910753880266076\n",
            "Epoch: 1, Batch: 500, Training Loss: 0.2622512399726464, Training Accuracy: 0.8929640718562875\n",
            "Epoch: 1, Batch: 550, Training Loss: 0.25888949280736556, Training Accuracy: 0.8949637023593466\n",
            "Epoch: 1, Batch: 600, Training Loss: 0.25581919918217993, Training Accuracy: 0.896630615640599\n",
            "Epoch: 1, Batch: 650, Training Loss: 0.2549126878289217, Training Accuracy: 0.8971774193548387\n",
            "Epoch: 1, Batch: 700, Training Loss: 0.24904384978312186, Training Accuracy: 0.9004992867332382\n",
            "Epoch: 1, Batch: 750, Training Loss: 0.24643729711335882, Training Accuracy: 0.901797603195739\n",
            "Epoch: 1, Batch: 800, Training Loss: 0.2455454660328419, Training Accuracy: 0.9019975031210986\n",
            "Epoch: 1, Batch: 850, Training Loss: 0.24506574027463532, Training Accuracy: 0.9024676850763808\n",
            "Epoch: 1, Batch: 900, Training Loss: 0.24302231595354293, Training Accuracy: 0.9037874583795783\n",
            "Epoch: 1, Batch: 950, Training Loss: 0.2411508617323327, Training Accuracy: 0.9048370136698213\n",
            "Epoch: 1, Batch: 1000, Training Loss: 0.2389443160456184, Training Accuracy: 0.9061563436563437\n",
            "Epoch: 1, Batch: 1050, Training Loss: 0.23599977296848845, Training Accuracy: 0.9071717411988582\n",
            "Epoch: 1, Batch: 1100, Training Loss: 0.23482367587863695, Training Accuracy: 0.907811080835604\n",
            "Epoch: 1, Batch: 1150, Training Loss: 0.2313167011506085, Training Accuracy: 0.9091550825369245\n",
            "Epoch: 1, Batch: 1200, Training Loss: 0.2299750674140642, Training Accuracy: 0.9094504579517069\n",
            "Epoch: 1, Batch: 1250, Training Loss: 0.22769208047107206, Training Accuracy: 0.9104716227018386\n",
            "Epoch: 1, Batch: 1300, Training Loss: 0.22665217052644074, Training Accuracy: 0.9109338970023059\n",
            "Epoch: 1, Batch: 1350, Training Loss: 0.22763510342607404, Training Accuracy: 0.9103904515173945\n",
            "Epoch: 1, Batch: 1400, Training Loss: 0.22575261318480475, Training Accuracy: 0.911134903640257\n",
            "Epoch: 1, Batch: 1450, Training Loss: 0.22415135589670224, Training Accuracy: 0.9116988283942109\n",
            "Epoch: 1, Batch: 1500, Training Loss: 0.22332494263273528, Training Accuracy: 0.9119753497668222\n",
            "Epoch: 1, Batch: 1550, Training Loss: 0.22267007651582102, Training Accuracy: 0.9122743391360413\n",
            "Epoch: 1, Batch: 1600, Training Loss: 0.22290931252656804, Training Accuracy: 0.9121252342286071\n",
            "Epoch: 1, Batch: 1650, Training Loss: 0.22287062116797624, Training Accuracy: 0.9120608721986675\n",
            "Epoch: 1, Batch: 1700, Training Loss: 0.22239190947235962, Training Accuracy: 0.9120370370370371\n",
            "Epoch: 1, Batch: 1750, Training Loss: 0.22081920013059333, Training Accuracy: 0.912835522558538\n",
            "Epoch: 1, Batch: 1800, Training Loss: 0.22040065880844173, Training Accuracy: 0.9129997223764575\n",
            "Epoch: 1, Batch: 1850, Training Loss: 0.2193910925740176, Training Accuracy: 0.9134589411129119\n",
            "Epoch: 1, Batch: 1900, Training Loss: 0.21878890203520743, Training Accuracy: 0.9135323513940031\n",
            "Epoch: 1, Batch: 1950, Training Loss: 0.21809658774822524, Training Accuracy: 0.9138903126601743\n",
            "Epoch: 1, Batch: 2000, Training Loss: 0.21737379566773304, Training Accuracy: 0.9143240879560219\n",
            "Epoch: 1, Batch: 2050, Training Loss: 0.217074087190227, Training Accuracy: 0.9144929302779132\n",
            "Epoch: 1, Batch: 2100, Training Loss: 0.21584936289621187, Training Accuracy: 0.9150107091861018\n",
            "Epoch: 1, Batch: 2150, Training Loss: 0.2149150632317505, Training Accuracy: 0.9155625290562529\n",
            "Epoch: 1, Batch: 2200, Training Loss: 0.2141884227246486, Training Accuracy: 0.9157769195820081\n",
            "Epoch: 1, Batch: 2250, Training Loss: 0.2142403871619512, Training Accuracy: 0.9155097734340293\n",
            "Epoch: 1, Batch: 2300, Training Loss: 0.21352708772972698, Training Accuracy: 0.9158246414602347\n",
            "Epoch: 1, Batch: 2350, Training Loss: 0.21339565282865808, Training Accuracy: 0.9159400255210549\n",
            "Epoch: 1, Batch: 2400, Training Loss: 0.2133970628222951, Training Accuracy: 0.9157902957101208\n",
            "Epoch: 1, Batch: 2450, Training Loss: 0.21259802458477248, Training Accuracy: 0.9160546715626275\n",
            "Epoch: 2, Batch: 0, Training Loss: 0.3101107180118561, Training Accuracy: 0.875\n",
            "Epoch: 2, Batch: 50, Training Loss: 0.15703329177317665, Training Accuracy: 0.9424019607843137\n",
            "Epoch: 2, Batch: 100, Training Loss: 0.14928837417449692, Training Accuracy: 0.9467821782178217\n",
            "Epoch: 2, Batch: 150, Training Loss: 0.13060314837991205, Training Accuracy: 0.9511589403973509\n",
            "Epoch: 2, Batch: 200, Training Loss: 0.12924751859223146, Training Accuracy: 0.9521144278606966\n",
            "Epoch: 2, Batch: 250, Training Loss: 0.13035721089728028, Training Accuracy: 0.953187250996016\n",
            "Epoch: 2, Batch: 300, Training Loss: 0.12636554776768252, Training Accuracy: 0.9559800664451827\n",
            "Epoch: 2, Batch: 350, Training Loss: 0.13009162671184776, Training Accuracy: 0.9538817663817664\n",
            "Epoch: 2, Batch: 400, Training Loss: 0.12983333180772322, Training Accuracy: 0.9543329177057357\n",
            "Epoch: 2, Batch: 450, Training Loss: 0.1318878246494001, Training Accuracy: 0.9537139689578714\n",
            "Epoch: 2, Batch: 500, Training Loss: 0.1297535927252409, Training Accuracy: 0.9543413173652695\n",
            "Epoch: 2, Batch: 550, Training Loss: 0.131520812017691, Training Accuracy: 0.9532667876588021\n",
            "Epoch: 2, Batch: 600, Training Loss: 0.13208452620607075, Training Accuracy: 0.9524750415973378\n",
            "Epoch: 2, Batch: 650, Training Loss: 0.13252952445181124, Training Accuracy: 0.9529569892473119\n",
            "Epoch: 2, Batch: 700, Training Loss: 0.13245183563247834, Training Accuracy: 0.9535485021398002\n",
            "Epoch: 2, Batch: 750, Training Loss: 0.13151001037171633, Training Accuracy: 0.9540612516644474\n",
            "Epoch: 2, Batch: 800, Training Loss: 0.13156244926010438, Training Accuracy: 0.954119850187266\n",
            "Epoch: 2, Batch: 850, Training Loss: 0.13270033518515623, Training Accuracy: 0.9534371327849589\n",
            "Epoch: 2, Batch: 900, Training Loss: 0.13287134166108186, Training Accuracy: 0.9528301886792453\n",
            "Epoch: 2, Batch: 950, Training Loss: 0.13365734216622954, Training Accuracy: 0.9529442691903259\n",
            "Epoch: 2, Batch: 1000, Training Loss: 0.13324274808646633, Training Accuracy: 0.952922077922078\n",
            "Epoch: 2, Batch: 1050, Training Loss: 0.13239615164443738, Training Accuracy: 0.9529614652711703\n",
            "Epoch: 2, Batch: 1100, Training Loss: 0.13274820943783416, Training Accuracy: 0.9525999091734787\n",
            "Epoch: 2, Batch: 1150, Training Loss: 0.1337681222536304, Training Accuracy: 0.9523240660295396\n",
            "Epoch: 2, Batch: 1200, Training Loss: 0.13497760845891058, Training Accuracy: 0.9516548709408826\n",
            "Epoch: 2, Batch: 1250, Training Loss: 0.13445474230367943, Training Accuracy: 0.9516886490807354\n",
            "Epoch: 2, Batch: 1300, Training Loss: 0.1347502252616011, Training Accuracy: 0.9514796310530361\n",
            "Epoch: 2, Batch: 1350, Training Loss: 0.13409759576565453, Training Accuracy: 0.9518412287194671\n",
            "Epoch: 2, Batch: 1400, Training Loss: 0.13413206500956507, Training Accuracy: 0.9517755174875089\n",
            "Epoch: 2, Batch: 1450, Training Loss: 0.13350299832993268, Training Accuracy: 0.9520589248793935\n",
            "Epoch: 2, Batch: 1500, Training Loss: 0.13392700071785993, Training Accuracy: 0.9518654230512992\n",
            "Epoch: 2, Batch: 1550, Training Loss: 0.13370462869660873, Training Accuracy: 0.9519261766602192\n",
            "Epoch: 2, Batch: 1600, Training Loss: 0.1347643198514663, Training Accuracy: 0.9515146783260462\n",
            "Epoch: 2, Batch: 1650, Training Loss: 0.1348847289006144, Training Accuracy: 0.9516580860084797\n",
            "Epoch: 2, Batch: 1700, Training Loss: 0.13521064347699766, Training Accuracy: 0.9514991181657848\n",
            "Epoch: 2, Batch: 1750, Training Loss: 0.13493504231664827, Training Accuracy: 0.951956025128498\n",
            "Epoch: 2, Batch: 1800, Training Loss: 0.13488631533859075, Training Accuracy: 0.9521099389228207\n",
            "Epoch: 2, Batch: 1850, Training Loss: 0.13414000662703818, Training Accuracy: 0.9521880064829822\n",
            "Epoch: 2, Batch: 1900, Training Loss: 0.13347882211604192, Training Accuracy: 0.9522619673855865\n",
            "Epoch: 2, Batch: 1950, Training Loss: 0.132407499650309, Training Accuracy: 0.9525884161968221\n",
            "Epoch: 2, Batch: 2000, Training Loss: 0.13234007795192843, Training Accuracy: 0.9525237381309345\n",
            "Epoch: 2, Batch: 2050, Training Loss: 0.13228944762074535, Training Accuracy: 0.9524012676743052\n",
            "Epoch: 2, Batch: 2100, Training Loss: 0.13116132683627776, Training Accuracy: 0.9527010947168015\n",
            "Epoch: 2, Batch: 2150, Training Loss: 0.13080641251751873, Training Accuracy: 0.9529579265457927\n",
            "Epoch: 2, Batch: 2200, Training Loss: 0.13184631785258535, Training Accuracy: 0.9528339391185825\n",
            "Epoch: 2, Batch: 2250, Training Loss: 0.13236459255671962, Training Accuracy: 0.9526876943580631\n",
            "Epoch: 2, Batch: 2300, Training Loss: 0.1324558618884438, Training Accuracy: 0.9526292916123424\n",
            "Epoch: 2, Batch: 2350, Training Loss: 0.13272337442575327, Training Accuracy: 0.9526531263292216\n",
            "Epoch: 2, Batch: 2400, Training Loss: 0.13251614164225897, Training Accuracy: 0.9527800916284881\n",
            "Epoch: 2, Batch: 2450, Training Loss: 0.13301854630607712, Training Accuracy: 0.9524173806609547\n",
            "Epoch: 3, Batch: 0, Training Loss: 0.11953361332416534, Training Accuracy: 0.9375\n",
            "Epoch: 3, Batch: 50, Training Loss: 0.06142925137343506, Training Accuracy: 0.9828431372549019\n",
            "Epoch: 3, Batch: 100, Training Loss: 0.0588630330702751, Training Accuracy: 0.9820544554455446\n",
            "Epoch: 3, Batch: 150, Training Loss: 0.059740288486081296, Training Accuracy: 0.9793046357615894\n",
            "Epoch: 3, Batch: 200, Training Loss: 0.062199928251614064, Training Accuracy: 0.9791666666666666\n",
            "Epoch: 3, Batch: 250, Training Loss: 0.07145407995760099, Training Accuracy: 0.976593625498008\n",
            "Epoch: 3, Batch: 300, Training Loss: 0.06948727164827473, Training Accuracy: 0.9769518272425249\n",
            "Epoch: 3, Batch: 350, Training Loss: 0.07223177235606697, Training Accuracy: 0.9763176638176638\n",
            "Epoch: 3, Batch: 400, Training Loss: 0.0682686551995436, Training Accuracy: 0.9777119700748129\n",
            "Epoch: 3, Batch: 450, Training Loss: 0.06899554236738338, Training Accuracy: 0.9776884700665188\n",
            "Epoch: 3, Batch: 500, Training Loss: 0.0710977541334823, Training Accuracy: 0.9760479041916168\n",
            "Epoch: 3, Batch: 550, Training Loss: 0.07364199769990806, Training Accuracy: 0.9751588021778584\n",
            "Epoch: 3, Batch: 600, Training Loss: 0.07416017789210808, Training Accuracy: 0.9750415973377704\n",
            "Epoch: 3, Batch: 650, Training Loss: 0.07378194659459082, Training Accuracy: 0.9750384024577573\n",
            "Epoch: 3, Batch: 700, Training Loss: 0.0725252907226638, Training Accuracy: 0.9750356633380884\n",
            "Epoch: 3, Batch: 750, Training Loss: 0.07499006779725848, Training Accuracy: 0.9743675099866844\n",
            "Epoch: 3, Batch: 800, Training Loss: 0.0756029509974861, Training Accuracy: 0.9744850187265918\n",
            "Epoch: 3, Batch: 850, Training Loss: 0.07562256221623641, Training Accuracy: 0.9741480611045829\n",
            "Epoch: 3, Batch: 900, Training Loss: 0.0764960293200298, Training Accuracy: 0.9735710321864595\n",
            "Epoch: 3, Batch: 950, Training Loss: 0.07580616914420649, Training Accuracy: 0.9739090431125131\n",
            "Epoch: 3, Batch: 1000, Training Loss: 0.0759370237861371, Training Accuracy: 0.974025974025974\n",
            "Epoch: 3, Batch: 1050, Training Loss: 0.07693338600472706, Training Accuracy: 0.9738344433872502\n",
            "Epoch: 3, Batch: 1100, Training Loss: 0.07740056803330507, Training Accuracy: 0.973830608537693\n",
            "Epoch: 3, Batch: 1150, Training Loss: 0.07679768933178889, Training Accuracy: 0.9739357080799305\n",
            "Epoch: 3, Batch: 1200, Training Loss: 0.07701088193365699, Training Accuracy: 0.9739800166527893\n",
            "Epoch: 3, Batch: 1250, Training Loss: 0.07801242521401014, Training Accuracy: 0.9737709832134293\n",
            "Epoch: 3, Batch: 1300, Training Loss: 0.07697832963670455, Training Accuracy: 0.9740584166026134\n",
            "Epoch: 3, Batch: 1350, Training Loss: 0.07627294257883468, Training Accuracy: 0.9744170984455959\n",
            "Epoch: 3, Batch: 1400, Training Loss: 0.07531712896299105, Training Accuracy: 0.9746609564596717\n",
            "Epoch: 3, Batch: 1450, Training Loss: 0.07687677440319697, Training Accuracy: 0.9741126809097175\n",
            "Epoch: 3, Batch: 1500, Training Loss: 0.0776582684799198, Training Accuracy: 0.9736842105263158\n",
            "Epoch: 3, Batch: 1550, Training Loss: 0.07770747848823793, Training Accuracy: 0.9735251450676983\n",
            "Epoch: 3, Batch: 1600, Training Loss: 0.07645908269094219, Training Accuracy: 0.9738835103060587\n",
            "Epoch: 3, Batch: 1650, Training Loss: 0.07783231108683077, Training Accuracy: 0.9735009085402786\n",
            "Epoch: 3, Batch: 1700, Training Loss: 0.07847112923292367, Training Accuracy: 0.9733245149911817\n",
            "Epoch: 3, Batch: 1750, Training Loss: 0.07860516614617986, Training Accuracy: 0.9731938892061679\n",
            "Epoch: 3, Batch: 1800, Training Loss: 0.07916687452754674, Training Accuracy: 0.9729664075513603\n",
            "Epoch: 3, Batch: 1850, Training Loss: 0.07896350186722814, Training Accuracy: 0.9729875742841707\n",
            "Epoch: 3, Batch: 1900, Training Loss: 0.0791469194930547, Training Accuracy: 0.9729089952656497\n",
            "Epoch: 3, Batch: 1950, Training Loss: 0.07918720244077836, Training Accuracy: 0.9728344438749359\n",
            "Epoch: 3, Batch: 2000, Training Loss: 0.07931747210794653, Training Accuracy: 0.9729510244877562\n",
            "Epoch: 3, Batch: 2050, Training Loss: 0.08040030221785711, Training Accuracy: 0.9725134080936129\n",
            "Epoch: 3, Batch: 2100, Training Loss: 0.08044088609014721, Training Accuracy: 0.9724833412660637\n",
            "Epoch: 3, Batch: 2150, Training Loss: 0.08017505034355676, Training Accuracy: 0.9725127847512784\n",
            "Epoch: 3, Batch: 2200, Training Loss: 0.08019463858484772, Training Accuracy: 0.9724840981372104\n",
            "Epoch: 3, Batch: 2250, Training Loss: 0.08028418273871983, Training Accuracy: 0.9724566859173701\n",
            "Epoch: 3, Batch: 2300, Training Loss: 0.08115817626152712, Training Accuracy: 0.972077357670578\n",
            "Epoch: 3, Batch: 2350, Training Loss: 0.08140574880711349, Training Accuracy: 0.9720331773713313\n",
            "Epoch: 3, Batch: 2400, Training Loss: 0.08119037257006, Training Accuracy: 0.9720949604331528\n",
            "Epoch: 3, Batch: 2450, Training Loss: 0.08196731293126078, Training Accuracy: 0.9717462260301918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the evaluation loop\n",
        "model.eval()\n",
        "total_val_loss = 0\n",
        "total_val_accuracy = 0\n",
        "\n",
        "# Iterate over the validation data\n",
        "for batch in val_dataloader:\n",
        "    # Move the batch to the device\n",
        "    batch_inputs = tuple(t.to(device) for t in batch)\n",
        "    inputs = {'input_ids': batch_inputs[0],\n",
        "              'attention_mask': batch_inputs[1],\n",
        "              'labels': batch_inputs[2]}\n",
        "\n",
        "    # Disable gradient calculation\n",
        "    with torch.no_grad():\n",
        "        # Perform the forward pass\n",
        "        outputs = model(**inputs)\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "    # Track the validation loss and accuracy\n",
        "    total_val_loss += loss.item()\n",
        "    preds = torch.argmax(logits, dim=1).flatten()\n",
        "    accuracy = (preds == inputs['labels']).float().mean()\n",
        "    total_val_accuracy += accuracy.item()\n",
        "\n",
        "# Calculate the average validation loss and accuracy\n",
        "avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "avg_val_accuracy = total_val_accuracy / len(val_dataloader)\n",
        "print(f'Validation Loss: {avg_val_loss}, Validation Accuracy: {avg_val_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbh5gdZSWbWf",
        "outputId": "081bc407-e9c4-4a45-ed1d-6466c8efc551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.23119196572685613, Validation Accuracy: 0.9324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have used seqence length we have used is 256 and batch size is 16."
      ],
      "metadata": {
        "id": "iJlXNbJMh9-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "# model_save_path = './xlnet_sentiment_analysis_model/'\n",
        "# if not os.path.exists(model_save_path):\n",
        "#     os.makedirs(model_save_path)\n",
        "\n",
        "# model.save_pretrained(model_save_path)\n",
        "# tokenizer.save_pretrained(model_save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dftQiKTaiFkG",
        "outputId": "8e85027c-1afc-4c29-aea6-dc19b256c7e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./xlnet_sentiment_analysis_model/tokenizer_config.json',\n",
              " './xlnet_sentiment_analysis_model/special_tokens_map.json',\n",
              " './xlnet_sentiment_analysis_model/spiece.model',\n",
              " './xlnet_sentiment_analysis_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# import shutil\n",
        "\n",
        "# # Zip the model directory\n",
        "# shutil.make_archive('xlnet_sentiment_analysis_model', 'zip', 'xlnet_sentiment_analysis_model')\n",
        "\n",
        "# # Download the zip file\n",
        "# files.download('xlnet_sentiment_analysis_model.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "yvQxBBvylAuv",
        "outputId": "41ec3135-cf31-4114-cae6-037aaf668d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_92543b08-8856-4c92-acfc-589987d7cc0d\", \"xlnet_sentiment_analysis_model.zip\", 436079861)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load the saved model and tokenizer\n",
        "# model_save_path = './xlnet_sentiment_analysis_model/'\n",
        "# model = XLNetForSequenceClassification.from_pretrained(model_save_path)\n",
        "# tokenizer = XLNetTokenizer.from_pretrained(model_save_path)"
      ],
      "metadata": {
        "id": "5f9yrh2rqnRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Set the model to evaluation mode\n",
        "# model.eval()\n",
        "\n",
        "# # Define variables to track the validation loss and accuracy\n",
        "# total_val_loss = 0\n",
        "# total_val_accuracy = 0\n",
        "\n",
        "# # Iterate over the validation data\n",
        "# for batch in val_dataloader:\n",
        "#     # Move the batch to the device\n",
        "#     batch_inputs = tuple(t.to(device) for t in batch)\n",
        "#     inputs = {'input_ids': batch_inputs[0],\n",
        "#               'attention_mask': batch_inputs[1],\n",
        "#               'labels': batch_inputs[2]}\n",
        "\n",
        "#     # Load the model parameters onto the same device as the input tensors\n",
        "#     model.to(device)\n",
        "\n",
        "#     # Disable gradient calculation\n",
        "#     with torch.no_grad():\n",
        "#         # Perform the forward pass\n",
        "#         outputs = model(**inputs)\n",
        "#         loss = outputs[0]\n",
        "#         logits = outputs[1]\n",
        "\n",
        "#     # Track the validation loss and accuracy\n",
        "#     total_val_loss += loss.item()\n",
        "#     preds = torch.argmax(logits, dim=1).flatten()\n",
        "#     accuracy = (preds == inputs['labels']).float().mean()\n",
        "#     total_val_accuracy += accuracy.item()\n",
        "\n",
        "# # Calculate the average validation loss and accuracy\n",
        "# avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "# avg_val_accuracy = total_val_accuracy / len(val_dataloader)\n",
        "\n",
        "# print(f'Validation Loss: {avg_val_loss}, Validation Accuracy: {avg_val_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwLW_u7zubq5",
        "outputId": "4ffbb249-0c55-42a5-f1f5-58219aad5d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 0.23119196572685613, Validation Accuracy: 0.9324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mUImwKRRumXh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}