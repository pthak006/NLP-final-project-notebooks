2023-03-14 19:33:21.812745: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-14 19:33:24.869572: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-03-14 19:33:24.869876: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-03-14 19:33:24.869896: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
[nltk_data] Downloading package punkt to /home/pthak006/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/home/pthak006/miniconda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']
- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.weight', 'logits_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/pthak006/miniconda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
                                              review  ...                                     stemmed_review
0  I love the movies and own the comics, the comi...  ...  i love the movi and own the comic , the comic ...
1  I first saw this film in 1980 in the midday mo...  ...  i first saw thi film in 1980 in the midday mov...
2  "The Last Big Thing" is a wonderful satirical ...  ...  `` the last big thing '' is a wonder satir fil...
3  I saw only the first part of this series when ...  ...  i saw onli the first part of thi seri when it ...
4  Until today I had never seen this film. Its wa...  ...  until today i had never seen thi film . it wa ...

[5 rows x 3 columns]
Total number of instances: 50000
Number of positive instances: 25000
Number of negative instances: 25000
Epoch: 1, Batch: 0, Training Loss: 0.6734170913696289, Training Accuracy: 0.5625
Epoch: 1, Batch: 50, Training Loss: 0.6980724194470573, Training Accuracy: 0.5355392156862745
Epoch: 1, Batch: 100, Training Loss: 0.6603012303314587, Training Accuracy: 0.5946782178217822
Epoch: 1, Batch: 150, Training Loss: 0.5653577848578131, Training Accuracy: 0.6771523178807947
Epoch: 1, Batch: 200, Training Loss: 0.5221745220212201, Training Accuracy: 0.7170398009950248
Epoch: 1, Batch: 250, Training Loss: 0.48725460677507865, Training Accuracy: 0.7452689243027888
Epoch: 1, Batch: 300, Training Loss: 0.46910887070470475, Training Accuracy: 0.7612126245847176
Epoch: 1, Batch: 350, Training Loss: 0.44984614679276774, Training Accuracy: 0.7765313390313391
Epoch: 1, Batch: 400, Training Loss: 0.43127459670391466, Training Accuracy: 0.7905236907730673
Epoch: 1, Batch: 450, Training Loss: 0.4155396831752588, Training Accuracy: 0.8000277161862528
Epoch: 1, Batch: 500, Training Loss: 0.409197809401446, Training Accuracy: 0.8032684630738522
Epoch: 1, Batch: 550, Training Loss: 0.3956719663960964, Training Accuracy: 0.8126134301270418
Epoch: 1, Batch: 600, Training Loss: 0.38369830775255964, Training Accuracy: 0.819779534109817
Epoch: 1, Batch: 650, Training Loss: 0.37432550135914083, Training Accuracy: 0.826036866359447
Epoch: 1, Batch: 700, Training Loss: 0.37014183715070875, Training Accuracy: 0.8296184022824536
Epoch: 1, Batch: 750, Training Loss: 0.36610618775360754, Training Accuracy: 0.8317243675099867
Epoch: 1, Batch: 800, Training Loss: 0.36217292365658743, Training Accuracy: 0.833723470661673
Epoch: 1, Batch: 850, Training Loss: 0.35789022216470484, Training Accuracy: 0.8361486486486487
Epoch: 1, Batch: 900, Training Loss: 0.35278978790843923, Training Accuracy: 0.839622641509434
Epoch: 1, Batch: 950, Training Loss: 0.34732351333811706, Training Accuracy: 0.8429942166140905
Epoch: 1, Batch: 1000, Training Loss: 0.3442025734781549, Training Accuracy: 0.8445304695304695
Epoch: 1, Batch: 1050, Training Loss: 0.3395714432800403, Training Accuracy: 0.846872026641294
Epoch: 1, Batch: 1100, Training Loss: 0.33624472226744534, Training Accuracy: 0.8486603088101726
Epoch: 1, Batch: 1150, Training Loss: 0.3330345233873206, Training Accuracy: 0.850510425716768
Epoch: 1, Batch: 1200, Training Loss: 0.3300488465004209, Training Accuracy: 0.8522064945878435
Epoch: 1, Batch: 1250, Training Loss: 0.328220575508162, Training Accuracy: 0.8529676258992805
Epoch: 1, Batch: 1300, Training Loss: 0.32461603436773667, Training Accuracy: 0.8549192928516526
Epoch: 1, Batch: 1350, Training Loss: 0.32228953707108426, Training Accuracy: 0.8563564026646928
Epoch: 1, Batch: 1400, Training Loss: 0.3192372828659513, Training Accuracy: 0.8580478229835832
Epoch: 1, Batch: 1450, Training Loss: 0.3166101496624308, Training Accuracy: 0.8591919365954515
Epoch: 1, Batch: 1500, Training Loss: 0.31554849120756634, Training Accuracy: 0.860093271152565
Epoch: 1, Batch: 1550, Training Loss: 0.3143670693199618, Training Accuracy: 0.8606544165054804
Epoch: 1, Batch: 1600, Training Loss: 0.31227642564576524, Training Accuracy: 0.8618831980012492
Epoch: 1, Batch: 1650, Training Loss: 0.31042842506213864, Training Accuracy: 0.862961841308298
Epoch: 1, Batch: 1700, Training Loss: 0.3092331713917438, Training Accuracy: 0.8637933568489125
Epoch: 1, Batch: 1750, Training Loss: 0.3078670203818731, Training Accuracy: 0.8644703026841805
Epoch: 1, Batch: 1800, Training Loss: 0.30660364527445594, Training Accuracy: 0.865352581898945
Epoch: 1, Batch: 1850, Training Loss: 0.30525875665189073, Training Accuracy: 0.8663222582387898
Epoch: 1, Batch: 1900, Training Loss: 0.3042339577899559, Training Accuracy: 0.8670436612309311
Epoch: 1, Batch: 1950, Training Loss: 0.3028994011866938, Training Accuracy: 0.8679523321373654
Epoch: 1, Batch: 2000, Training Loss: 0.3011882230914061, Training Accuracy: 0.8690342328835582
Epoch: 1, Batch: 2050, Training Loss: 0.2995341080978789, Training Accuracy: 0.869728181374939
Epoch: 1, Batch: 2100, Training Loss: 0.29756255066337, Training Accuracy: 0.8707758210376011
Epoch: 1, Batch: 2150, Training Loss: 0.2957506737156989, Training Accuracy: 0.8717166434216643
Epoch: 1, Batch: 2200, Training Loss: 0.2949335983827852, Training Accuracy: 0.8724443434802363
Epoch: 1, Batch: 2250, Training Loss: 0.2939810849443204, Training Accuracy: 0.8728898267436694
Epoch: 1, Batch: 2300, Training Loss: 0.2934210486944825, Training Accuracy: 0.8729900043459365
Epoch: 1, Batch: 2350, Training Loss: 0.2917420084083945, Training Accuracy: 0.8738302849851127
Epoch: 1, Batch: 2400, Training Loss: 0.2907354865170416, Training Accuracy: 0.8747396917950854
Epoch: 1, Batch: 2450, Training Loss: 0.29110614876922103, Training Accuracy: 0.8748215014279885
Epoch: 2, Batch: 0, Training Loss: 0.23896625638008118, Training Accuracy: 0.9375
Epoch: 2, Batch: 50, Training Loss: 0.17367760281936795, Training Accuracy: 0.9350490196078431
Epoch: 2, Batch: 100, Training Loss: 0.19120411444824226, Training Accuracy: 0.9238861386138614
Epoch: 2, Batch: 150, Training Loss: 0.19072972276920316, Training Accuracy: 0.9279801324503312
Epoch: 2, Batch: 200, Training Loss: 0.18542546469403143, Training Accuracy: 0.9284825870646766
Epoch: 2, Batch: 250, Training Loss: 0.1839391442898854, Training Accuracy: 0.9320219123505976
Epoch: 2, Batch: 300, Training Loss: 0.1840684288289658, Training Accuracy: 0.9318936877076412
Epoch: 2, Batch: 350, Training Loss: 0.18321396985178828, Training Accuracy: 0.9325142450142451
Epoch: 2, Batch: 400, Training Loss: 0.18755845361832818, Training Accuracy: 0.9311097256857855
Epoch: 2, Batch: 450, Training Loss: 0.1870445463467728, Training Accuracy: 0.9309866962305987
Epoch: 2, Batch: 500, Training Loss: 0.18608826292518846, Training Accuracy: 0.9316367265469062
Epoch: 2, Batch: 550, Training Loss: 0.1876997691283371, Training Accuracy: 0.9310344827586207
Epoch: 2, Batch: 600, Training Loss: 0.18533766770676496, Training Accuracy: 0.9314683860232945
Epoch: 2, Batch: 650, Training Loss: 0.18510177019502846, Training Accuracy: 0.9315476190476191
Epoch: 2, Batch: 700, Training Loss: 0.18496338193160022, Training Accuracy: 0.9310805991440799
Epoch: 2, Batch: 750, Training Loss: 0.18550616323813976, Training Accuracy: 0.9304260985352862
Epoch: 2, Batch: 800, Training Loss: 0.18362899222521673, Training Accuracy: 0.9311017478152309
Epoch: 2, Batch: 850, Training Loss: 0.18285591258928968, Training Accuracy: 0.9314042303172738
Epoch: 2, Batch: 900, Training Loss: 0.18073037176389872, Training Accuracy: 0.9320199778024417
Epoch: 2, Batch: 950, Training Loss: 0.17943023743997127, Training Accuracy: 0.932570977917981
Epoch: 2, Batch: 1000, Training Loss: 0.18009185372385664, Training Accuracy: 0.9318806193806194
Epoch: 2, Batch: 1050, Training Loss: 0.178149786693612, Training Accuracy: 0.9326831588962893
Epoch: 2, Batch: 1100, Training Loss: 0.17893119097825183, Training Accuracy: 0.9321071752951862
Epoch: 2, Batch: 1150, Training Loss: 0.1798643129194021, Training Accuracy: 0.9318527367506516
Epoch: 2, Batch: 1200, Training Loss: 0.17990619541579342, Training Accuracy: 0.9318796835970025
Epoch: 2, Batch: 1250, Training Loss: 0.1806436460956538, Training Accuracy: 0.9312549960031974
Epoch: 2, Batch: 1300, Training Loss: 0.17914189737029573, Training Accuracy: 0.9318793235972329
Epoch: 2, Batch: 1350, Training Loss: 0.17971985120130504, Training Accuracy: 0.9312546262028127
Epoch: 2, Batch: 1400, Training Loss: 0.1803415420104209, Training Accuracy: 0.9309421841541756
Epoch: 2, Batch: 1450, Training Loss: 0.1803841012639785, Training Accuracy: 0.931038938662991
Epoch: 2, Batch: 1500, Training Loss: 0.1798838821936555, Training Accuracy: 0.9312541638907396
Epoch: 2, Batch: 1550, Training Loss: 0.1806265446666669, Training Accuracy: 0.9308107672469375
Epoch: 2, Batch: 1600, Training Loss: 0.18086866515314556, Training Accuracy: 0.9305512179887571
Epoch: 2, Batch: 1650, Training Loss: 0.18030331260566998, Training Accuracy: 0.9308752271350696
Epoch: 2, Batch: 1700, Training Loss: 0.180999754169197, Training Accuracy: 0.9307760141093474
Epoch: 2, Batch: 1750, Training Loss: 0.17997286005955146, Training Accuracy: 0.9310037121644774
Epoch: 2, Batch: 1800, Training Loss: 0.18007929454547772, Training Accuracy: 0.9309064408661855
Epoch: 2, Batch: 1850, Training Loss: 0.18056729508021485, Training Accuracy: 0.9308144246353323
Epoch: 2, Batch: 1900, Training Loss: 0.1809407286517428, Training Accuracy: 0.9305957390846923
Epoch: 2, Batch: 1950, Training Loss: 0.18081112386852594, Training Accuracy: 0.9308047155304972
Epoch: 2, Batch: 2000, Training Loss: 0.18143693854271814, Training Accuracy: 0.9305347326336831
Epoch: 2, Batch: 2050, Training Loss: 0.18170099208737178, Training Accuracy: 0.9304302779132131
Epoch: 2, Batch: 2100, Training Loss: 0.18222389775135514, Training Accuracy: 0.9303010471204188
Epoch: 2, Batch: 2150, Training Loss: 0.18298646098938876, Training Accuracy: 0.9300615992561599
Epoch: 2, Batch: 2200, Training Loss: 0.18236495552035298, Training Accuracy: 0.9301737846433439
Epoch: 2, Batch: 2250, Training Loss: 0.1825104047085466, Training Accuracy: 0.9301421590404265
Epoch: 2, Batch: 2300, Training Loss: 0.18237650234686104, Training Accuracy: 0.9300847457627118
Epoch: 2, Batch: 2350, Training Loss: 0.18260813304526166, Training Accuracy: 0.9299500212675458
Epoch: 2, Batch: 2400, Training Loss: 0.18292941524841508, Training Accuracy: 0.9298469387755102
Epoch: 2, Batch: 2450, Training Loss: 0.18266668393850777, Training Accuracy: 0.9300030599755202
Epoch: 3, Batch: 0, Training Loss: 0.025322822853922844, Training Accuracy: 1.0
Epoch: 3, Batch: 50, Training Loss: 0.13461581287065558, Training Accuracy: 0.9497549019607843
Epoch: 3, Batch: 100, Training Loss: 0.11414687754355814, Training Accuracy: 0.9603960396039604
Epoch: 3, Batch: 150, Training Loss: 0.11082877877279376, Training Accuracy: 0.9581953642384106
Epoch: 3, Batch: 200, Training Loss: 0.11049181228823293, Training Accuracy: 0.9583333333333334
Epoch: 3, Batch: 250, Training Loss: 0.10892951623462438, Training Accuracy: 0.9596613545816733
Epoch: 3, Batch: 300, Training Loss: 0.11402412278475581, Training Accuracy: 0.9586794019933554
Epoch: 3, Batch: 350, Training Loss: 0.11162103831959076, Training Accuracy: 0.9586894586894587
Epoch: 3, Batch: 400, Training Loss: 0.11281087407911954, Training Accuracy: 0.9585411471321695
Epoch: 3, Batch: 450, Training Loss: 0.11178298770117588, Training Accuracy: 0.9588414634146342
Epoch: 3, Batch: 500, Training Loss: 0.1112174139517971, Training Accuracy: 0.9592065868263473
Epoch: 3, Batch: 550, Training Loss: 0.11448957032681242, Training Accuracy: 0.9582577132486388
Epoch: 3, Batch: 600, Training Loss: 0.11671268482291187, Training Accuracy: 0.9571547420965059
Epoch: 3, Batch: 650, Training Loss: 0.12080512114209674, Training Accuracy: 0.9554531490015361
Epoch: 3, Batch: 700, Training Loss: 0.12077593697253589, Training Accuracy: 0.9555099857346647
Epoch: 3, Batch: 750, Training Loss: 0.11940401505192769, Training Accuracy: 0.9560585885486018
Epoch: 3, Batch: 800, Training Loss: 0.12276988252113746, Training Accuracy: 0.9545880149812734
Epoch: 3, Batch: 850, Training Loss: 0.12202851810371099, Training Accuracy: 0.9545387779083431
Epoch: 3, Batch: 900, Training Loss: 0.12182716106077021, Training Accuracy: 0.9549112097669257
Epoch: 3, Batch: 950, Training Loss: 0.12137117854820918, Training Accuracy: 0.955310199789695
Epoch: 3, Batch: 1000, Training Loss: 0.12199405515468777, Training Accuracy: 0.9548576423576424
Epoch: 3, Batch: 1050, Training Loss: 0.12272973795249513, Training Accuracy: 0.9549833491912464
Epoch: 3, Batch: 1100, Training Loss: 0.12204795626760186, Training Accuracy: 0.9551544050862852
Epoch: 3, Batch: 1150, Training Loss: 0.12290349720126029, Training Accuracy: 0.9547675933970461
Epoch: 3, Batch: 1200, Training Loss: 0.1225173331571853, Training Accuracy: 0.954673189009159
Epoch: 3, Batch: 1250, Training Loss: 0.1229468973293576, Training Accuracy: 0.9546862509992007
Epoch: 3, Batch: 1300, Training Loss: 0.1229872042053722, Training Accuracy: 0.9546983089930823
Epoch: 3, Batch: 1350, Training Loss: 0.12279650539026729, Training Accuracy: 0.9547557364914878
Epoch: 3, Batch: 1400, Training Loss: 0.12358131666825971, Training Accuracy: 0.9543629550321199
Epoch: 3, Batch: 1450, Training Loss: 0.12395938577131721, Training Accuracy: 0.9543849069607168
Epoch: 3, Batch: 1500, Training Loss: 0.12426077818397499, Training Accuracy: 0.9542388407728182
Epoch: 3, Batch: 1550, Training Loss: 0.1247204108929909, Training Accuracy: 0.9539813023855577
Epoch: 3, Batch: 1600, Training Loss: 0.12418875090186272, Training Accuracy: 0.954286383510306
Epoch: 3, Batch: 1650, Training Loss: 0.12463140836098925, Training Accuracy: 0.9537780133252575
Epoch: 3, Batch: 1700, Training Loss: 0.12405986235359862, Training Accuracy: 0.9539609053497943
Epoch: 3, Batch: 1750, Training Loss: 0.12434132879603652, Training Accuracy: 0.9538121073672188
Epoch: 3, Batch: 1800, Training Loss: 0.12528314769719476, Training Accuracy: 0.9534286507495836
Epoch: 3, Batch: 1850, Training Loss: 0.1258661400106384, Training Accuracy: 0.9532347379794706
Epoch: 3, Batch: 1900, Training Loss: 0.126349664482948, Training Accuracy: 0.9532154129405576
Epoch: 3, Batch: 1950, Training Loss: 0.125854331763129, Training Accuracy: 0.9533892875448488
Epoch: 3, Batch: 2000, Training Loss: 0.12565306358601153, Training Accuracy: 0.9535232383808095
Epoch: 3, Batch: 2050, Training Loss: 0.1255559794029683, Training Accuracy: 0.9536201852754754
Epoch: 3, Batch: 2100, Training Loss: 0.1259207271556124, Training Accuracy: 0.9535935268919562
Epoch: 3, Batch: 2150, Training Loss: 0.12554435991605908, Training Accuracy: 0.9537424453742446
Epoch: 3, Batch: 2200, Training Loss: 0.1262872154416744, Training Accuracy: 0.9532598818718764
Epoch: 3, Batch: 2250, Training Loss: 0.12642197957316598, Training Accuracy: 0.9531041759218125
Epoch: 3, Batch: 2300, Training Loss: 0.12704569482129602, Training Accuracy: 0.9528194263363755
Epoch: 3, Batch: 2350, Training Loss: 0.12690828318877193, Training Accuracy: 0.9528923862186304
Epoch: 3, Batch: 2400, Training Loss: 0.12655292636346768, Training Accuracy: 0.9529883381924198
Epoch: 3, Batch: 2450, Training Loss: 0.12659457214127257, Training Accuracy: 0.952952876376989
Validation Loss: 0.25588198541626334, Validation Accuracy: 0.9149
